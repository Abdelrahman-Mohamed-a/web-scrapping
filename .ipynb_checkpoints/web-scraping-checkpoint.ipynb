{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "31df61bf-f8a9-4ce5-8e88-efa49768a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.baraasallout.com/test.html\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import csv\n",
    "import json  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d0eb3ab4-d11e-401f-87a5-41e7f4fa8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 1\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "data  = requests.get(url).text \n",
    "soup = BeautifulSoup(data,\"html.parser\")  \n",
    "all_h1 = soup.find_all('h1')\n",
    "all_h2 = soup.find_all('h2')\n",
    "all_p = soup.find_all('p')\n",
    "all_li = soup.find_all('li')\n",
    "\n",
    "file = open('./csvs/Extract_Text_Data.csv','w')\n",
    "writer = csv.writer(file)\n",
    "for h1 in all_h1:\n",
    "    writer.writerow(['Heading',h1.text])\n",
    "for h2 in all_h2:\n",
    "    writer.writerow(['Heading',h2.text])\n",
    "for p in all_p:\n",
    "    writer.writerow(['Paragraph',p.text])\n",
    "    \n",
    "for li in all_li:\n",
    "    writer.writerow(['Paragraph',li.text])\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "717b7738-8cd2-4456-909b-2c86a387f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 2\n",
    "table = soup.find('table')\n",
    "rows = table.find_all('tr')\n",
    "data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all(['td', 'th'])\n",
    "    data.append([col.text.strip() for col in cols])\n",
    "\n",
    "file = open('./csvs/Extract_Table_Data.csv','w')\n",
    "writer = csv.writer(file)\n",
    "writer.writerows(data)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "849aa685-ea46-4ae3-abd5-e9ed911d08f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./jsons/Product_Information.csv was written successfully\n"
     ]
    }
   ],
   "source": [
    "# task 3\n",
    "file = open('./jsons/Product_Information.json','w')\n",
    "writer = csv.writer(file)\n",
    "h2_tags = soup.find_all('h2')\n",
    "fourth_h2 = h2_tags[3] \n",
    "productsDiv = fourth_h2.find_next('div')\n",
    "\n",
    "rows = productsDiv.find_all(\"div\")\n",
    "data = []\n",
    "for row in rows:\n",
    "    all_p = row.find_all('p')\n",
    "    title = all_p[0].find_all('strong')[0].text\n",
    "    price = all_p[1].text\n",
    "    stock_availability = all_p[2].text\n",
    "    button_text = row.find('button').text\n",
    "    data.append({'title':title,'price':price,'stock_availability':stock_availability,'button_text':button_text})\n",
    "json.dump(data, file)\n",
    "file.close()\n",
    "print('./jsons/Product_Information.csv was written successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "664869ba-7829-425d-8b5d-d155096f94d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 4\n",
    "all_inputs = soup.find_all('input')\n",
    "data = []\n",
    "for input_ in all_inputs:\n",
    "    name = input_.get('name')\n",
    "    type_ = input_.get('type')\n",
    "    default = input_.get('placeholder')\n",
    "    if(name):\n",
    "        data.append({'name':name,'type':type_,'default':default})\n",
    "    \n",
    "file = open('./jsons/Form_Details.json','w')\n",
    "json.dump(data, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c9ba9cd6-c878-4444-a0af-4917a4c52c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 5\n",
    "all_hyperlink= soup.find_all('a')\n",
    "all_iframes = soup.find_all('iframe')\n",
    "\n",
    "data = []\n",
    "for link_ in all_hyperlink:\n",
    "    href = link_.get('href')\n",
    "    data.append({'hyperlink':href})\n",
    "for iframe_ in all_iframes:\n",
    "    src = iframe_.get('src')\n",
    "    data.append({'video_link':src})\n",
    "\n",
    "file = open('./jsons/Links_Multimedia.json','w')\n",
    "json.dump(data, file)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4ed08b6b-2147-4d41-9172-661c91fd7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 6\n",
    "file = open('./jsons/Featured_Products.json','w')\n",
    "h2_tags = soup.find_all('h2')\n",
    "fourth_h2 = h2_tags[5] \n",
    "products = fourth_h2.find_next('div').find_all('div')\n",
    "data = []\n",
    "for product in products:\n",
    "    id = product.get('data-id')\n",
    "    name = product.find('p',class_='name').text\n",
    "    price = product.find('p',class_='price').text\n",
    "    colors = product.find('p',class_='colors').text.split(\": \")[1]\n",
    "    product_details ={\"id\":id,\"name\":name,\"price\":price,\"colors\":colors}\n",
    "    data.append(product_details)\n",
    "json.dump(data, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad57f8-5e4d-42ff-9d36-057f05b22af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
